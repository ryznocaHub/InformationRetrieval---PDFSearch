{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization local dokumen\n",
    "def allFile(location):\n",
    "    document = []\n",
    "    for doc in os.walk(location):\n",
    "        document = doc[2]\n",
    "    return document\n",
    "\n",
    "def extractPDF(location):\n",
    "    documents = allFile(location)\n",
    "    allText = []\n",
    "    for doc in documents:\n",
    "        file = open(location+'/'+doc, 'rb')\n",
    "        fileReader = PyPDF2.PdfFileReader(file)\n",
    "        \n",
    "        docs = ''\n",
    "        pages = fileReader.numPages\n",
    "        for page in range(pages):\n",
    "            obj = fileReader.getPage(page)\n",
    "            docs = docs + obj.extractText()\n",
    "        allText.append(docs)\n",
    "    return allText\n",
    "\n",
    "def generateDocNumber(filename):\n",
    "    docNum = []\n",
    "    for file in filename:\n",
    "        docNum.append(str(filename.index(file)))\n",
    "    return docNum\n",
    "\n",
    "# PREPROCESSING\n",
    "def removePunctuation(textList):\n",
    "    for i in range(len(textList)):\n",
    "        for punct in string.punctuation:\n",
    "            textList[i] = textList[i].replace(punct, \" \")\n",
    "        textList[i] = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', textList[i], flags=re.MULTILINE)\n",
    "    return textList\n",
    "\n",
    "def caseFolding(textList):\n",
    "    text = []\n",
    "    for i in range(len(textList)):\n",
    "        text.append(textList[i].lower())\n",
    "    return text\n",
    "\n",
    "def token(sentence):\n",
    "    token = []\n",
    "    for word in CountVectorizer().build_tokenizer()(sentence):\n",
    "        token.append(word)\n",
    "    return token\n",
    "\n",
    "def tokenize(textList):\n",
    "    tokens = []\n",
    "    for i in range(len(textList)):\n",
    "        tokens.append(token(textList[i]))\n",
    "    return tokens\n",
    "\n",
    "def checkStopword(sentence, stop_words):\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    return sentence\n",
    "    \n",
    "def stopwordRemove(textList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = []\n",
    "    for i in range(len(textList)):\n",
    "        text.append(checkStopword(textList[i], stop_words))\n",
    "    return text\n",
    "\n",
    "def numberRemove(textList):\n",
    "    text = []\n",
    "    for i in range(len(textList)):\n",
    "        text.append([w for w in textList[i] if not any(j.isdigit() for j in w)])\n",
    "    return text\n",
    "\n",
    "def stemming(textList):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = textList\n",
    "    for i in range(len(textList)):\n",
    "        for j in range(len(textList[i])):\n",
    "            text[i][j] = stemmer.stem(text[i][j])\n",
    "    return text\n",
    "\n",
    "def sorting(textList):\n",
    "    for i in range(len(textList)):\n",
    "        textList[i] = sorted(textList[i])\n",
    "    return textList\n",
    "\n",
    "def getAllTerms(textList):\n",
    "    terms = []\n",
    "    for i in range(len(textList)):\n",
    "        for j in range(len(textList[i])):\n",
    "            terms.append(textList[i][j])\n",
    "    return sorted(set(terms))\n",
    "\n",
    "# INDEXING \n",
    "def createIndex(textList, docno):\n",
    "    terms = getAllTerms(textList)\n",
    "    proximity = {}\n",
    "    for term in terms:\n",
    "        position = {}\n",
    "        for n in range(len(textList)):\n",
    "            if(term in textList[n]):\n",
    "                position[docno[n]] = []\n",
    "                for i in range(len(textList[n])):\n",
    "                    if(term == textList[n][i]):\n",
    "                        position[docno[n]].append(i)\n",
    "        proximity[str(term)] = position\n",
    "    return proximity\n",
    "\n",
    "def exportIndex(index, filename):\n",
    "    file = open(filename,'w')\n",
    "    for n in index:\n",
    "        file.write(str(n)+'\\n')\n",
    "        for o in index[n]:\n",
    "            file.write('\\t'+o+': ')\n",
    "            for p in range(len(index[n][o])):\n",
    "                file.write(str(index[n][o][p]))\n",
    "                if(p<len(index[n][o])-1):\n",
    "                    file.write(', ')\n",
    "                else:\n",
    "                    file.write('\\n')\n",
    "    file.close()\n",
    "    return \"file index berhasil dibuat.\"    \n",
    "\n",
    "# Ranking document\n",
    "def queryInIndex(query, index):\n",
    "    result = []\n",
    "    for word in query:\n",
    "        if word in index:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "def df(query, index):\n",
    "    docFreq = {}\n",
    "    for word in query:\n",
    "        if word in index:\n",
    "            docFreq[word] = len(index[word])\n",
    "    return docFreq\n",
    "\n",
    "def idf(df, N):\n",
    "    inv = {}\n",
    "    for word in df:\n",
    "        inv[word] = math.log10(N/df[word])\n",
    "    return inv\n",
    "\n",
    "def tf(query, index):\n",
    "    termFreq = {}\n",
    "    for word in query:\n",
    "        freq = {}\n",
    "        if word in index:\n",
    "            for i in index[word]:\n",
    "                freq[i] = len(index[word][i])\n",
    "        termFreq[word] = freq\n",
    "    return termFreq\n",
    "\n",
    "def tfidf(tf, idf):\n",
    "    w = {}\n",
    "    for word in tf:\n",
    "        wtd = {}\n",
    "        for doc in tf[word]:\n",
    "            wtd[doc] = (1+(math.log10(tf[word][doc])))*idf[word]\n",
    "        w[word] = wtd\n",
    "    return w\n",
    "    \n",
    "def score(TFIDF):\n",
    "    res = {}\n",
    "    for i in TFIDF:\n",
    "        for j in TFIDF[i]:\n",
    "            res[j] = 0\n",
    "    for i in TFIDF:\n",
    "        for j in TFIDF[i]:\n",
    "            res[j] = res[j]+TFIDF[i][j]\n",
    "    sorted_dict = sorted(res, key=res.get, reverse=True)\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file index berhasil dibuat.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = 'pdf_collection'\n",
    "filename = allFile(location) \n",
    "extracted= extractPDF(location)\n",
    "totalDoc = len(filename)\n",
    "documentNumber = generateDocNumber(filename)\n",
    "\n",
    "for i in range(len(filename)):\n",
    "    extracted[i] = str(extracted[i].encode(\"utf-8\"))\n",
    "    \n",
    "# PREPROCESSING\n",
    "text = removePunctuation(extracted)\n",
    "text = caseFolding(text)\n",
    "text = tokenize(text)\n",
    "text = stopwordRemove(text)\n",
    "text = numberRemove(text)\n",
    "text = stemming(text)\n",
    "\n",
    "# GET ALL TERMS IN COLLECTION\n",
    "terms = getAllTerms(text)\n",
    "\n",
    "# INDEXING\n",
    "\n",
    "# index = createIndex(text,documentNumber, terms)\n",
    "index = createIndex(text,documentNumber)\n",
    "\n",
    "# CREATE INDEX FILE\n",
    "exportIndex(index, 'Index_docs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY\n",
    "\n",
    "raw_query = [\"database\"]\n",
    "\n",
    "query = removePunctuation(raw_query)\n",
    "query = caseFolding(query)\n",
    "query = tokenize(query)\n",
    "query = stopwordRemove(query)\n",
    "query = numberRemove(query)\n",
    "query = stemming(query)\n",
    "query = query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "query = queryInIndex(query, index)\n",
    "N               = totalDoc\n",
    "tfidf_list      = []\n",
    "\n",
    "docFrequency    = df(query, index)\n",
    "invDocFrequency = idf(docFrequency, N)\n",
    "termFrequency   = tf(query, index)\n",
    "TFIDF           = tfidf(termFrequency, invDocFrequency)\n",
    "sc              = score(TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['database'] \n",
      "\n",
      "\n",
      "Result: \n",
      "\n",
      "==========================================================================\n",
      "\n",
      "| Filename:  Database Auditing.pdf  | Document ID:  3 | \n",
      "\n",
      "b 1 Database Auditing  Jungha Woo  Sael Lee  and Carla Zoltowski  n wooj  lee399  cbz  purdue edu  n Abstract  n nGovernment regulations and increased awareness of  nsecurity issues have  nincreased the auditing  nrequirements of information technology systems   In  n nthis paper  we will discuss three government  nregulations and how they have impacted  ninformation techno nlogy systems   We classify  ndatabase auditing systems by considering features  nof the basic components of an auditing system as  n nproposed by Bishop  the logger  analyzer  and  nnotifier   In addition  we will consider possible  npolicy models that could be implemented   Finally  n nwe will survey three commerci nal database and third  nparty auditing products according to the  nclassification features  and discuss how they address  n nthe government regulation ns and general security  nneeds       n 1  Introduction  n Auditing is the recording and analyzing of events or  nstatistics to provide information abou\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "| Filename:  [22] Jungha Woo, Sael Lee, and Carla Zoltowski _ Database auditing.pdf  | Document ID:  15 | \n",
      "\n",
      "b 1 Database Auditing  Jungha Woo  Sael Lee  and Carla Zoltowski  n wooj  lee399  cbz  purdue edu  n Abstract  n nGovernment regulations and increased awareness of  nsecurity issues have  nincreased the auditing  nrequirements of information technology systems   In  n nthis paper  we will discuss three government  nregulations and how they have impacted  ninformation techno nlogy systems   We classify  ndatabase auditing systems by considering features  nof the basic components of an auditing system as  n nproposed by Bishop  the logger  analyzer  and  nnotifier   In addition  we will consider possible  npolicy models that could be implemented   Finally  n nwe will survey three commerci nal database and third  nparty auditing products according to the  nclassification features  and discuss how they address  n nthe government regulation ns and general security  nneeds       n 1  Introduction  n Auditing is the recording and analyzing of events or  nstatistics to provide information abou\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "| Filename:  yang2009.pdf  | Document ID:  12 | \n",
      "\n",
      "b Teaching Database Security and Auditing Li Yang Department of Computer Science and Engineering  University of Tennessee at Chattanooga   Chattanooga  TN 37403  Li Yang utc edu  ABSTRACT Hands on laboratory experiences ar ne essential critical for students to understand concepts and gain real world insights in database security and auditing    We are  ndeveloping a set of hands on labs  nto integrate theories of database n security into practices   Our  ndesigned labs do not require purchasing any commercial software  nor pre configuration   Each lab includes objectives  results  and  nresources to help students to understand database security  nconcepts including access control  virtual private database  and  n ndatabase auditing etc   We us ne two major database products  Microsoft SQL Server and Oracle 10g  to design and implement  n nour labs        nCategories and Subject Descriptors K 3 2  Computer and Information Science Education n    nComputer science education  Curriculum  H\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "| Filename:  [19] Qiang Huang, Lianzhong Liu, A Logging Scheme for Database Audit.pdf  | Document ID:  13 | \n",
      "\n",
      "b A Logging Scheme for Database Audit  n Qiang Huang  Lianzhong Liu  nSchool of Computer Science and Engineering  Key Laboratory of Beijing Network Technology  nBeihang University  nBeijing  China  nhqbuaa gmail com  lz liu buaa edu cn  n  Abstract n xc5 xa0Database audit can strengthen the security of  ndatabase  Logging database activities is usually the first step of  n nimplementing database audit  In this paper  we present a  nlogging scheme for database audit  Unlike native database  nlogging and auditing mechanism  our scheme is to monitor and  n nlog database activities through analyzing network traffic  The  n narchitecture of our scheme contains three principal  ncomponents  packets capturing  packets parsing and data  nstorage  First capture the packets to and from the database   n nthen  by analyzing database communication protocols  parse  n nthe captured packets  finally  use the parsed results to support  ndatabase audit   nKeywords logging  database audit  network traff\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "\n",
      "| Filename:  A Framework for Database Auditing.pdf  | Document ID:  0 | \n",
      "\n",
      "b A Framework for Database Auditing  n  nLianzhong Liu  Qiang Huang  nSchool of Computer Science and Engineering  Key Laboratory of Beijing Network Technology  nBeihang University  nBeijing  China  nlz liu buaa edu cn  hqbuaa gmail com  n  n  nAbstract n xc5 xa0Database auditing can help strengthen the security of  ndatabase  In this paper  we present a framework of database  nauditing  which log the database activities through analyzing  nnetwork traffic  execute audit analysis through event correlation  nand generate alarms if an anomaly or a violation of security  nregulations is detected  Compared with native auditing  nmechanism in database  our approach has an obvious advantage  nof providing zero impact to the performance of the database or  nthe applications that access it  In addition  using third party  nauditing component complies with the principle of separation of  nduties   nKeywords dat nabase auditing  loggin ng  audit analysis   nI  n  n  nI nNTRODUCTION n  nDatabases \n",
      "\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "\n",
    "print('Query: ', raw_query,'\\n\\n')\n",
    "print('Result: \\n')\n",
    "# for i in range(5):\n",
    "#     a = documentNumber.index(sc[i])\n",
    "#     print('Document Number: ',sc[i])\n",
    "#     print(filename[a])\n",
    "#     print('-------------------------------------------\\n')\n",
    "\n",
    "count = 0\n",
    "for i in range(len(sc)):\n",
    "    a = documentNumber.index(sc[i])\n",
    "    print('==========================================================================\\n')\n",
    "    print('| Filename: ',filename[a],' | Document ID: ',documentNumber[a],'|','\\n')\n",
    "    print(extracted[a][0:1000])\n",
    "    print('\\n==========================================================================')\n",
    "    print('\\n\\n\\n')\n",
    "    count = count + 1\n",
    "    if(count >= 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
